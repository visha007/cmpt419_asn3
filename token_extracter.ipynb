{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d2e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, random\n",
    "import gzip, json\n",
    "from io import BytesIO\n",
    "\n",
    "#2 step randomization (option1 discussed inclass)\n",
    "#   2.1 - extract a random list of urls from dolma \n",
    "#   2.2 - filter out urls containing unsafe files (refinedweb ones)\n",
    "#   2.3 - true random sampling of 15 urls to extract tokens from (next cell)\n",
    "\n",
    "#full list of Dolma dataset files are here...\n",
    "dolma_url_list = \"https://huggingface.co/datasets/allenai/dolma/raw/main/urls/v1_7.txt\"\n",
    "\n",
    "#download url list via python request\n",
    "resp = requests.get(dolma_url_list)\n",
    "all_urls = resp.text.splitlines()  #break list into separate urls like [ 'https://url1', 'https://url2', ..., 'https://urln']\n",
    "\n",
    "#filter out unsafe or unwanted sources\n",
    "safe_urls = [u for u in all_urls if \"falcon-refinedweb\" not in u]\n",
    "\n",
    "#randomly sample 15\n",
    "sampled_urls = random.sample(safe_urls, 15)\n",
    "# print(\"Selected urls:\\n\")\n",
    "# for u in sampled_urls:\n",
    "#     print(u)\n",
    "\n",
    "#stream 300k tokens from 15 randomly selected urls \n",
    "token_target = 300000\n",
    "token_count = 0\n",
    "tokens_collected = [] #stores the actual text samples extracted in an array\n",
    "\n",
    "#loop runs until the token_target is reached\n",
    "# - the above usually fulfilled from 1st file in loop\n",
    "# - since a different random sample of files is selected each time \n",
    "#       -- high possibility that a different file is being tokenized each time - 2nd layer of randomization\n",
    "for i, data_url in enumerate(sampled_urls):\n",
    "    print(f\"\\nReading file at ->>> {data_url}\")\n",
    "    resp = requests.get(data_url, stream=True)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    #open compressed stream\n",
    "    with gzip.open(BytesIO(resp.content), \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                text = obj.get(\"text\", \"\")\n",
    "                tokens = len(text.split())\n",
    "                token_count += tokens\n",
    "                tokens_collected.append(text)\n",
    "\n",
    "                if token_count >= token_target:\n",
    "                    print(f\"\\nGood job! Reached target. Collected {token_count} tokens\")\n",
    "                    break\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    if token_count >= token_target:\n",
    "        break\n",
    "\n",
    "print(f\"\\nCollected {len(tokens_collected)} text samples from (~{token_count} tokens).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcff4f",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f6a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view collected text\n",
    "print(\"\\nExtracted text samples\\n\")\n",
    "for i, sample in enumerate(tokens_collected, 1):\n",
    "    print(f\"sample #{i}:\\n{sample}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
